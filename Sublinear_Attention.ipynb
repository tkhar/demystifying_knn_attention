{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_attention(Q, K, V):\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Scale the attention scores by the square root of the dimension of the key vectors\n",
    "    attention_scores = attention_scores / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
    "    \n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Multiply the attention weights with the value vectors\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.0000,  -2.0000,  -4.0000,  ...,  -2.0000,   6.0000,  -6.0000],\n",
      "        [ -5.0000,  -3.0000, -10.0000,  ...,  -1.0000,   1.0000,   7.0000],\n",
      "        [ -1.4879,  -0.0237,   2.0490,  ...,   4.3646,   1.0478,   1.0248],\n",
      "        ...,\n",
      "        [ -5.9965,  -6.0050,  -1.0085,  ...,  -5.9949,  -1.0034,   8.9879],\n",
      "        [ -3.0000,   3.0000,  -4.0000,  ..., -10.0000,  -5.0000,  -2.0000],\n",
      "        [ -8.9995,  -8.9964,  -4.9981,  ...,  -1.9978,   9.9990,   6.9994]])\n"
     ]
    }
   ],
   "source": [
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (50, 50)).float()\n",
    "K = torch.randint(-10, 11, (50, 50)).float()\n",
    "V = torch.randint(-10, 11, (50, 50)).float()\n",
    "\n",
    "# Call the calculate_attention function\n",
    "attention_output = calculate_attention(Q, K, V)\n",
    "\n",
    "# Print the attention output\n",
    "print(attention_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Find top-k indices of inner products q^T k_i for a given query q and set of keys K\n",
    "# \n",
    "def topk_indices(Q, K, k):\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "    # Apply the exponential function to the attention scores\n",
    "    attention_scores = torch.exp(attention_scores)\n",
    "    \n",
    "    # Find the top-k indices of the attention scores\n",
    "    topk_scores, topk_indices = torch.topk(attention_scores, k, dim=-1)\n",
    "    \n",
    "    return topk_scores, topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[162754.7969,  22026.4648]]), tensor([[1, 0]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1]])\n",
    "\n",
    "topk_indices(q, K, 2)  # Output: tensor([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#\n",
    "# Approximating the softmax partition function for a single query using the top-k method\n",
    "# q is the query vector\n",
    "# K is the matrix of key vectors: n x d\n",
    "# k is the number of top-k elements to consider\n",
    "# l is the number of samples to draw from the remaining elements\n",
    "# \n",
    "def approximate_softmax(q, K, k, l):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Find the top-k indices of the attention scores\n",
    "    scores, indices = topk_indices(q, K, k)\n",
    "\n",
    "    # Take the partition sum of the top-k attention scores\n",
    "    partition_sum = scores.sum(dim=-1)\n",
    "\n",
    "    # From the n-k remaining elements, draw l samples.\n",
    "    remaining_indices = set(range(n)) - set(indices[0].tolist())\n",
    "\n",
    "    # Randomly sample l indices from the remaining elements\n",
    "    random_indices = numpy.random.choice(list(remaining_indices), l, replace=False)\n",
    "\n",
    "    # Calculate the attention scores of the remaining elements\n",
    "    remaining_sum = torch.exp(torch.matmul(q, K[random_indices, :].transpose(-2, -1)))\n",
    "\n",
    "    # Return the approximate softmax partition function.\n",
    "    return (n-k) * (remaining_sum.sum() / l) + partition_sum.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3315263.5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3457876.2500)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1], [2, 3, 4, 5], [6, 7, 8, 9]])\n",
    "print(approximate_softmax(q, K, 1, 2))\n",
    "\n",
    "# Compare with the true softmax partition function\n",
    "attention_scores = torch.exp(torch.matmul(q, K.transpose(-2, -1)))\n",
    "true_softmax = attention_scores.sum()\n",
    "true_softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Approximating attention with top-k method\n",
    "# \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
