{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_attention(Q, K, V, B):\n",
    "    n, d = Q.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Numerical stability\n",
    "    attention_scores -= B*B\n",
    "    \n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Multiply the attention weights with the value vectors\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5545e+00, -1.0099e+00, -7.7820e+00,  ...,  6.8297e+00,\n",
      "          7.6803e+00,  7.6429e+00],\n",
      "        [-7.4637e+00, -4.9500e+00,  4.2071e+00,  ...,  5.0288e+00,\n",
      "         -6.3750e-03, -7.4135e+00],\n",
      "        [ 1.1425e+00, -5.0924e-01, -1.3612e+00,  ...,  1.9775e+00,\n",
      "         -4.6699e+00,  1.2864e+00],\n",
      "        ...,\n",
      "        [ 4.9724e+00,  3.8048e-02, -9.8822e+00,  ...,  4.9183e+00,\n",
      "          3.0044e+00,  4.9205e+00],\n",
      "        [ 5.7137e+00, -4.0068e+00,  1.7566e+00,  ..., -3.7334e-01,\n",
      "          2.4066e+00,  2.5427e+00],\n",
      "        [-2.9471e+00,  1.0142e+00, -5.5731e+00,  ...,  6.0247e+00,\n",
      "          8.5180e+00, -1.2846e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (50, 50)).float()\n",
    "K = torch.randint(-10, 11, (50, 50)).float()\n",
    "V = torch.randint(-10, 11, (50, 50)).float()\n",
    "\n",
    "# Call the calculate_attention function\n",
    "attention_output = calculate_attention(Q, K, V, B=10.0)\n",
    "\n",
    "# Print the attention output\n",
    "print(attention_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Find top-k indices of inner products q^T k_i for every query q and set of keys K\n",
    "# This is done naively here, in O(n^2) time\n",
    "# \n",
    "def topk_indices_naive(Q, K, k, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Apply the exponential function to the attention scores\n",
    "    attention_scores = torch.exp(attention_scores - B * B)\n",
    "    \n",
    "    # Find the top-k indices of the attention scores\n",
    "    topk_scores, topk_indices = torch.topk(attention_scores, k, dim=-1)\n",
    "    \n",
    "    return topk_scores, topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.4689e-43, 4.5262e-43]]), tensor([[1, 0]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1]])\n",
    "\n",
    "topk_indices_naive(q, K, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#\n",
    "# Approximating the expected value of a value vector v with underlying distribution\n",
    "# p(i) = softmax(q^T k_i) / Z, where Z is the partition function\n",
    "# \n",
    "# Parameters:\n",
    "#   q is the query vector: 1 x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   v is the value vector: 1 x n\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is a function that returns the top-k indices of the attention scores\n",
    "#   B is the maximum value of the q,k,v elements. We use this to avoid numerical instability.\n",
    "# \n",
    "def approximate_softmax_expectation(q, K, v, k, l, topk_indices_func, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Find the top-k indices of the attention scores\n",
    "    scores, indices = topk_indices_func(q, K, k, B)\n",
    "\n",
    "    # From the n-k remaining elements, draw l samples.\n",
    "    # TODO: Implement this in O(l) time.\n",
    "    remaining_indices = set(range(n)) - set(indices)\n",
    "\n",
    "    # Randomly sample l indices from the remaining elements\n",
    "    random_indices = numpy.random.choice(list(remaining_indices), l, replace=False)\n",
    "\n",
    "    # Now we'll evaluate the partition function and the expectation separately.\n",
    "\n",
    "    approx_partition = 0\n",
    "    approx_expectation = 0\n",
    "    for index in random_indices:\n",
    "        # Calculate the attention score for the remaining elements\n",
    "        attention_score = torch.exp(torch.dot(q, K[index]) / d - B*B)\n",
    "\n",
    "        # Add the attention score to the partition function\n",
    "        approx_partition += attention_score\n",
    "\n",
    "        # Add the attention score times the value to the expectation\n",
    "        approx_expectation += attention_score * v[index]\n",
    "\n",
    "    approx_partition *= ((n-k) / l)\n",
    "    approx_expectation *= ((n-k) / l)\n",
    "\n",
    "    approx_partition += scores.sum()\n",
    "    approx_expectation += torch.sum(scores * v[indices[0]])\n",
    "\n",
    "    # Return the approximate softmax partition function.\n",
    "    return approx_expectation / approx_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Approximates the attention output using sampling.\n",
    "# \n",
    "# Parameters:\n",
    "#   Q is the matrix of query vectors: n x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   V is the matrix of value vectors: n x d\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is the function to find the top-k indices\n",
    "def sampling_attention(Q, K, V, k, l, topk_indices_func, B):\n",
    "\n",
    "    output = torch.zeros_like(V)\n",
    "\n",
    "    # For all rows in Q...\n",
    "    for i in range(Q.size(0)): # n\n",
    "        # For all columns in V...\n",
    "        for j in range(V.size(1)): # d\n",
    "            # Approximate the expected value of the value vector\n",
    "            output[i, j] = approximate_softmax_expectation(Q[i], K, V[i], k, l, topk_indices_func, B)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for approximate attention: 43.3422908782959\n",
      "Time taken for exact attention: 0.008199930191040039\n",
      "Mean error:  tensor(4.2154)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (200, 200)).float()\n",
    "K = torch.randint(-10, 11, (200, 200)).float()\n",
    "V = torch.randint(-10, 11, (200, 200)).float()\n",
    "\n",
    "# Call the sampling_attention function.\n",
    "# Measure the time it takes to run the function.\n",
    "start_time = time.time()\n",
    "attention_output = sampling_attention(Q, K, V, 50, 50, topk_indices_naive, B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for approximate attention:\", end_time - start_time)\n",
    "\n",
    "# Compare with the exact attention output\n",
    "# Calculate the time taken to run the calculate_attention function.\n",
    "start_time = time.time()\n",
    "exact_attention_output = calculate_attention(Q, K, V,B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for exact attention:\", end_time - start_time)\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(\"Mean error: \", torch.mean(torch.abs(attention_output - exact_attention_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the top-k inner products q^T k_i for a query q and a set of \n",
    "# keys K. It does so in O(k) time by using the concentric circle LSH idea.\n",
    "def topk_indices_fast(q, K, k, B):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
