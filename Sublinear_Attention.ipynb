{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_attention(Q, K, V, B):\n",
    "    n, d = Q.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Numerical stability\n",
    "    attention_scores -= B*B\n",
    "    \n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Multiply the attention weights with the value vectors\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5545e+00, -1.0099e+00, -7.7820e+00,  ...,  6.8297e+00,\n",
      "          7.6803e+00,  7.6429e+00],\n",
      "        [-7.4637e+00, -4.9500e+00,  4.2071e+00,  ...,  5.0288e+00,\n",
      "         -6.3750e-03, -7.4135e+00],\n",
      "        [ 1.1425e+00, -5.0924e-01, -1.3612e+00,  ...,  1.9775e+00,\n",
      "         -4.6699e+00,  1.2864e+00],\n",
      "        ...,\n",
      "        [ 4.9724e+00,  3.8048e-02, -9.8822e+00,  ...,  4.9183e+00,\n",
      "          3.0044e+00,  4.9205e+00],\n",
      "        [ 5.7137e+00, -4.0068e+00,  1.7566e+00,  ..., -3.7334e-01,\n",
      "          2.4066e+00,  2.5427e+00],\n",
      "        [-2.9471e+00,  1.0142e+00, -5.5731e+00,  ...,  6.0247e+00,\n",
      "          8.5180e+00, -1.2846e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (50, 50)).float()\n",
    "K = torch.randint(-10, 11, (50, 50)).float()\n",
    "V = torch.randint(-10, 11, (50, 50)).float()\n",
    "\n",
    "# Call the calculate_attention function\n",
    "attention_output = calculate_attention(Q, K, V, B=10.0)\n",
    "\n",
    "# Print the attention output\n",
    "print(attention_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Find top-k indices of inner products q^T k_i for every query q and set of keys K\n",
    "# This is done naively here, in O(n^2) time\n",
    "# \n",
    "def topk_indices_naive(Q, K, k, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Apply the exponential function to the attention scores\n",
    "    attention_scores = torch.exp(attention_scores - B * B)\n",
    "    \n",
    "    # Find the top-k indices of the attention scores\n",
    "    topk_scores, topk_indices = torch.topk(attention_scores, k, dim=-1)\n",
    "    \n",
    "    return topk_scores, topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.4689e-43, 4.5262e-43]]), tensor([[1, 0]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1]])\n",
    "\n",
    "topk_indices_naive(q, K, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#\n",
    "# Approximating the expected value of a value vector v with underlying distribution\n",
    "# p(i) = softmax(q^T k_i) / Z, where Z is the partition function\n",
    "# \n",
    "# Parameters:\n",
    "#   q is the query vector: 1 x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   v is the value vector: 1 x n\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is a function that returns the top-k indices of the attention scores\n",
    "#   B is the maximum value of the q,k,v elements. We use this to avoid numerical instability.\n",
    "# \n",
    "def approximate_softmax_expectation(q, K, v, k, l, topk_indices_func, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Find the top-k indices of the attention scores\n",
    "    scores, indices = topk_indices_func(q, K, k, B)\n",
    "\n",
    "    # From the n-k remaining elements, draw l samples.\n",
    "    # TODO: Implement this in O(l) time.\n",
    "    remaining_indices = set(range(n)) - set(indices)\n",
    "\n",
    "    # Randomly sample l indices from the remaining elements\n",
    "    random_indices = numpy.random.choice(list(remaining_indices), l, replace=False)\n",
    "\n",
    "    # Now we'll evaluate the partition function and the expectation separately.\n",
    "\n",
    "    approx_partition = 0\n",
    "    approx_expectation = 0\n",
    "    for index in random_indices:\n",
    "        # Calculate the attention score for the remaining elements\n",
    "        attention_score = torch.exp(torch.dot(q, K[index]) / d - B*B)\n",
    "\n",
    "        # Add the attention score to the partition function\n",
    "        approx_partition += attention_score\n",
    "\n",
    "        # Add the attention score times the value to the expectation\n",
    "        approx_expectation += attention_score * v[index]\n",
    "\n",
    "    approx_partition *= ((n-k) / l)\n",
    "    approx_expectation *= ((n-k) / l)\n",
    "\n",
    "    approx_partition += scores.sum()\n",
    "    approx_expectation += torch.sum(scores * v[indices[0]])\n",
    "\n",
    "    # Return the approximate softmax partition function.\n",
    "    return approx_expectation / approx_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Approximates the attention output using sampling.\n",
    "# \n",
    "# Parameters:\n",
    "#   Q is the matrix of query vectors: n x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   V is the matrix of value vectors: n x d\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is the function to find the top-k indices\n",
    "def sampling_attention(Q, K, V, k, l, topk_indices_func, B):\n",
    "\n",
    "    output = torch.zeros_like(V)\n",
    "\n",
    "    # For all rows in Q...\n",
    "    for i in range(Q.size(0)): # n\n",
    "        # For all columns in V...\n",
    "        for j in range(V.size(1)): # d\n",
    "            # Approximate the expected value of the value vector\n",
    "            output[i, j] = approximate_softmax_expectation(Q[i], K, V[i], k, l, topk_indices_func, B)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for approximate attention: 43.3422908782959\n",
      "Time taken for exact attention: 0.008199930191040039\n",
      "Mean error:  tensor(4.2154)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (200, 200)).float()\n",
    "K = torch.randint(-10, 11, (200, 200)).float()\n",
    "V = torch.randint(-10, 11, (200, 200)).float()\n",
    "\n",
    "# Call the sampling_attention function.\n",
    "# Measure the time it takes to run the function.\n",
    "start_time = time.time()\n",
    "attention_output = sampling_attention(Q, K, V, 50, 50, topk_indices_naive, B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for approximate attention:\", end_time - start_time)\n",
    "\n",
    "# Compare with the exact attention output\n",
    "# Calculate the time taken to run the calculate_attention function.\n",
    "start_time = time.time()\n",
    "exact_attention_output = calculate_attention(Q, K, V,B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for exact attention:\", end_time - start_time)\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(\"Mean error: \", torch.mean(torch.abs(attention_output - exact_attention_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class angularLSH:\n",
    "    def __init__ (self, K, r, c, B):\n",
    "        self.K = K\n",
    "        self.c = c\n",
    "        self.r = r\n",
    "        self.n, self.d = K.size()\n",
    "\n",
    "        # Normalize the key vectors by dividing with d:\n",
    "        self.K /= self.d\n",
    "\n",
    "        # Normalize further by dividing all the key vectors by B*B\n",
    "        self.K /= (B*B)\n",
    "\n",
    "        # Now add an extra dimension to the key vectors to \n",
    "        # make them all have norm 1. Now all the key vectors have d+1 dimensions.\n",
    "        self.K = torch.cat((self.K, torch.sqrt(1 - torch.sum(self.K**2, dim=1, keepdim=True))), dim=1)\n",
    "        self.d += 1\n",
    "\n",
    "        # LSH works as follows:\n",
    "        # 1. To hash a single vector, concatenate k = O(log n) smaller hashes. \n",
    "        #    That's one hash table. That means we have 2^k = O(n) buckets\n",
    "        # 2. Maintain L = n^s hash tables where s = log(1-arccos(b)/pi) / log(1-arccos(s)/pi)\n",
    "        self.k = int(torch.ceil(torch.log2(self.n)))\n",
    "        self.L = torch.log(1-torch.arccos(self.c * self.r)/torch.pi) / torch.log(1-torch.arccos(self.r)/torch.pi)\n",
    "\n",
    "        # A hash is just the sign of the dot product of the vector \n",
    "        # with a random vector on the unit sphere.\n",
    "        # (Coordinates are drawn from a Gaussian distribution.)\n",
    "        self.hash_vectors = torch.randn(self.L, self.k, self.d)\n",
    "\n",
    "        # We'll store L hash tables, each with 2^k buckets. Each bucket\n",
    "        # will store a list of indices of the key vectors.\n",
    "        self.hash_tables = [{} for _ in range(self.L)]\n",
    "\n",
    "        # Now we hash all the key vectors L times:\n",
    "        for i in range(self.n):\n",
    "            ki = self.K[i] # Get the i-th key vector: 1 x d\n",
    "\n",
    "            # Compute the hash of the key vector for all L hash tables.\n",
    "            for j in range(self.L):\n",
    "                hj = self.hash(ki, self.hash_vectors[j]) # number between 0 and 2^k - 1\n",
    "\n",
    "                # Add the index of the key vector to the corresponding bucket.\n",
    "                if hj not in self.hash_tables[j]:\n",
    "                    self.hash_tables[j][hj] = []\n",
    "\n",
    "                self.hash_tables[j][hj].append(i)\n",
    "\n",
    "    #\n",
    "    # Hash a single vector x using the hash function h.\n",
    "    # h consists of k random vectors on the unit sphere.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #  x is the input vector: 1 x (d+1)\n",
    "    #  h is the hash function: k x (d+1)\n",
    "    # \n",
    "    # Returns:\n",
    "    #  The hash of the input vector x: a number between 0 and 2^k - 1\n",
    "    # \n",
    "    def hash(self, x, h):\n",
    "        # If the input vector x has d dimensions, we add an extra dimension of 0.\n",
    "        # This is needed for the query vectors, but not the key vectors because\n",
    "        # we already added an extra dimension to them.\n",
    "        if len(x) == self.d - 1:\n",
    "            x = torch.cat((x, torch.tensor([0.0])))\n",
    "\n",
    "        hash_value = 0\n",
    "        for i in range(self.k):\n",
    "            if torch.dot(x, h[i]) >= 0:\n",
    "                hash_value += 2**i\n",
    "\n",
    "        return hash_value\n",
    "\n",
    "    # Function to query how many (and which) key vectors are in the same bucket \n",
    "    # as the query vector q.\n",
    "    # \n",
    "    # We limit the number of returned key vectors to max_results.\n",
    "    # We also require the keys to have a dot product of at most cr with the query vector.\n",
    "    #\n",
    "    # Runtime: O(max(max_results, number of keys in the bucket))\n",
    "    def query_bucket_size(self, q, max_results):\n",
    "\n",
    "        # If the query vector q has d dimensions, we add an extra dimension of 0.\n",
    "        if len(q) == self.d - 1:\n",
    "            q = torch.cat((q, torch.tensor([0.0])))\n",
    "\n",
    "        distinct_keys = set()\n",
    "        for j in range(self.L):\n",
    "            hj = self.hash(q, self.hash_vectors[j])\n",
    "\n",
    "            if hj in self.hash_tables[j]:\n",
    "                for key_index in self.hash_tables[j][hj]:\n",
    "\n",
    "                    # Check if the key vector has a dot product of at most cr with the query vector.\n",
    "                    if torch.dot(q, self.K[key_index]) >= self.c * self.r:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add the key index to the set of distinct keys.\n",
    "                    distinct_keys.add(key_index)\n",
    "\n",
    "                    # If we have reached the maximum number of results, return the keys.\n",
    "                    if len(distinct_keys) >= max_results:\n",
    "                        return distinct_keys\n",
    "\n",
    "        return distinct_keys, len(distinct_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a sequence of concentric LSH objects with increasing r values.\n",
    "def topk_indices_lsh_preprocessing(K, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    c = d/(B*B*B*B)\n",
    "    current_s = 0\n",
    "    current_b = c * s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the top-k inner products q^T k_i for a query q and a set of \n",
    "# keys K. It does so in O(k) time by using the concentric circle LSH idea.\n",
    "def topk_indices_fast_lsh(q, K, k, B):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
