{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_attention(Q, K, V, B):\n",
    "    n, d = Q.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Numerical stability\n",
    "    attention_scores -= B*B\n",
    "    \n",
    "    # Apply softmax to get the attention weights\n",
    "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Multiply the attention weights with the value vectors\n",
    "    attention_output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5545e+00, -1.0099e+00, -7.7820e+00,  ...,  6.8297e+00,\n",
      "          7.6803e+00,  7.6429e+00],\n",
      "        [-7.4637e+00, -4.9500e+00,  4.2071e+00,  ...,  5.0288e+00,\n",
      "         -6.3750e-03, -7.4135e+00],\n",
      "        [ 1.1425e+00, -5.0924e-01, -1.3612e+00,  ...,  1.9775e+00,\n",
      "         -4.6699e+00,  1.2864e+00],\n",
      "        ...,\n",
      "        [ 4.9724e+00,  3.8048e-02, -9.8822e+00,  ...,  4.9183e+00,\n",
      "          3.0044e+00,  4.9205e+00],\n",
      "        [ 5.7137e+00, -4.0068e+00,  1.7566e+00,  ..., -3.7334e-01,\n",
      "          2.4066e+00,  2.5427e+00],\n",
      "        [-2.9471e+00,  1.0142e+00, -5.5731e+00,  ...,  6.0247e+00,\n",
      "          8.5180e+00, -1.2846e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (50, 50)).float()\n",
    "K = torch.randint(-10, 11, (50, 50)).float()\n",
    "V = torch.randint(-10, 11, (50, 50)).float()\n",
    "\n",
    "# Call the calculate_attention function\n",
    "attention_output = calculate_attention(Q, K, V, B=10.0)\n",
    "\n",
    "# Print the attention output\n",
    "print(attention_output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Find top-k indices of inner products q^T k_i for every query q and set of keys K\n",
    "# This is done naively here, in O(n^2) time\n",
    "# \n",
    "def topk_indices_naive(Q, K, k, B, lsh_objects=None):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Calculate the dot product of Q and K\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
    "\n",
    "    # Normalize the attention scores by dividing by d.\n",
    "    attention_scores = attention_scores / d\n",
    "\n",
    "    # Apply the exponential function to the attention scores\n",
    "    attention_scores = torch.exp(attention_scores - B * B)\n",
    "    \n",
    "    # Find the top-k indices of the attention scores\n",
    "    topk_scores, topk_indices = torch.topk(attention_scores, k, dim=-1)\n",
    "    \n",
    "    return topk_scores, topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.4689e-43, 4.5262e-43]]), tensor([[1, 0]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1]])\n",
    "\n",
    "topk_indices_naive(q, K, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#\n",
    "# Approximating the expected value of a value vector v with underlying distribution\n",
    "# p(i) = softmax(q^T k_i) / Z, where Z is the partition function\n",
    "# \n",
    "# Parameters:\n",
    "#   q is the query vector: 1 x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   v is the value vector: 1 x n\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is a function that returns the top-k indices of the attention scores\n",
    "#   B is the maximum value of the q,k,v elements. We use this to avoid numerical instability.\n",
    "# \n",
    "def approximate_softmax_expectation(q, K, v, k, l, topk_indices_func, B, lsh_objects=None):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # Find the top-k indices of the attention scores\n",
    "    scores, indices = topk_indices_func(q, K, k, B, lsh_objects)\n",
    "\n",
    "    # From the n-k remaining elements, draw l samples.\n",
    "    # TODO: Implement this in O(l) time.\n",
    "    remaining_indices = set(range(n)) - set(indices)\n",
    "\n",
    "    # Randomly sample l indices from the remaining elements\n",
    "    random_indices = numpy.random.choice(list(remaining_indices), l, replace=False)\n",
    "\n",
    "    # Now we'll evaluate the partition function and the expectation separately.\n",
    "\n",
    "    approx_partition = 0\n",
    "    approx_expectation = 0\n",
    "    for index in random_indices:\n",
    "        # Calculate the attention score for the remaining elements\n",
    "        attention_score = torch.exp(torch.dot(q, K[index]) / d - B*B)\n",
    "\n",
    "        # Add the attention score to the partition function\n",
    "        approx_partition += attention_score\n",
    "\n",
    "        # Add the attention score times the value to the expectation\n",
    "        approx_expectation += attention_score * v[index]\n",
    "\n",
    "    approx_partition *= ((n-k) / l)\n",
    "    approx_expectation *= ((n-k) / l)\n",
    "\n",
    "    approx_partition += scores.sum()\n",
    "    approx_expectation += torch.sum(scores * v[indices[0]])\n",
    "\n",
    "    # Return the approximate softmax partition function.\n",
    "    return approx_expectation / approx_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Approximates the attention output using sampling.\n",
    "# \n",
    "# Parameters:\n",
    "#   Q is the matrix of query vectors: n x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   V is the matrix of value vectors: n x d\n",
    "#   k is the number of top-k elements to consider\n",
    "#   l is the number of samples to draw from the remaining elements\n",
    "#   topk_indices_func is the function to find the top-k indices\n",
    "def sampling_attention(Q, K, V, k, l, topk_indices_func, B, lsh_objects=None):\n",
    "\n",
    "    output = torch.zeros_like(V)\n",
    "\n",
    "    # For all rows in Q...\n",
    "    for i in range(Q.size(0)): # n\n",
    "        # For all columns in V...\n",
    "        for j in range(V.size(1)): # d\n",
    "            # Approximate the expected value of the value vector\n",
    "            output[i, j] = approximate_softmax_expectation(Q[i], K, V[i], k, l, topk_indices_func, B, lsh_objects)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for approximate attention: 45.084949016571045\n",
      "Time taken for exact attention: 0.007837057113647461\n",
      "Mean error:  tensor(4.4101)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (200, 200)).float()\n",
    "K = torch.randint(-10, 11, (200, 200)).float()\n",
    "V = torch.randint(-10, 11, (200, 200)).float()\n",
    "\n",
    "# Call the sampling_attention function.\n",
    "# Measure the time it takes to run the function.\n",
    "start_time = time.time()\n",
    "attention_output = sampling_attention(Q, K, V, 50, 50, topk_indices_naive, B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for approximate attention:\", end_time - start_time)\n",
    "\n",
    "# Compare with the exact attention output\n",
    "# Calculate the time taken to run the calculate_attention function.\n",
    "start_time = time.time()\n",
    "exact_attention_output = calculate_attention(Q, K, V,B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for exact attention:\", end_time - start_time)\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(\"Mean error: \", torch.mean(torch.abs(attention_output - exact_attention_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class angularLSH:\n",
    "    def __init__ (self, K, s, b, B):\n",
    "        self.K = K\n",
    "        self.s = s\n",
    "        self.b = b\n",
    "        self.n, self.d = K.size()\n",
    "\n",
    "        # Normalize the key vectors by dividing with d:\n",
    "        self.K = self.K / self.d\n",
    "\n",
    "        # Normalize further by dividing all the key vectors by B*B\n",
    "        self.K = self.K / (B*B)\n",
    "\n",
    "        # Now add an extra dimension to the key vectors to \n",
    "        # make them all have norm 1. Now all the key vectors have d+1 dimensions.\n",
    "        self.K = torch.cat((self.K, torch.sqrt(1 - torch.sum(self.K**2, dim=1, keepdim=True))), dim=1)\n",
    "        self.d += 1\n",
    "\n",
    "        # LSH works as follows:\n",
    "        # 1. To hash a single vector, concatenate k = O(log n) smaller hashes. \n",
    "        #    That's one hash table. That means we have 2^k = O(n) buckets\n",
    "        # 2. Maintain L = n^s hash tables where s = log(1-arccos(b)/pi) / log(1-arccos(s)/pi)\n",
    "        self.k = int(math.ceil(math.log2(self.n)))\n",
    "        self.L = math.log2(1-np.arccos(self.b)/math.pi) / math.log2(1-np.arccos(self.s)/math.pi)\n",
    "        print(f\"Power of L: {self.L}\")\n",
    "        self.L = int(math.pow(self.n, self.L))\n",
    "\n",
    "        print(f\"Creating {self.L} hash tables with {self.k} bits each.\")\n",
    "\n",
    "        # A hash is just the sign of the dot product of the vector \n",
    "        # with a random vector on the unit sphere.\n",
    "        # (Coordinates are drawn from a Gaussian distribution.)\n",
    "        self.hash_vectors = torch.randn(self.L, self.k, self.d)\n",
    "\n",
    "        # We'll store L hash tables, each with 2^k buckets. Each bucket\n",
    "        # will store a list of indices of the key vectors.\n",
    "        self.hash_tables = [{} for _ in range(self.L)]\n",
    "\n",
    "        # Now we hash all the key vectors L times:\n",
    "        for i in range(self.n):\n",
    "            ki = self.K[i] # Get the i-th key vector: 1 x d\n",
    "\n",
    "            # Compute the hash of the key vector for all L hash tables.\n",
    "            for j in range(self.L):\n",
    "                hj = self.hash(ki, self.hash_vectors[j]) # number between 0 and 2^k - 1\n",
    "\n",
    "                # Add the index of the key vector to the corresponding bucket.\n",
    "                if hj not in self.hash_tables[j]:\n",
    "                    self.hash_tables[j][hj] = []\n",
    "\n",
    "                self.hash_tables[j][hj].append(i)\n",
    "\n",
    "    #\n",
    "    # Hash a single vector x using the hash function h.\n",
    "    # h consists of k random vectors on the unit sphere.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #  x is the input vector: 1 x (d+1)\n",
    "    #  h is the hash function: k x (d+1)\n",
    "    # \n",
    "    # Returns:\n",
    "    #  The hash of the input vector x: a number between 0 and 2^k - 1\n",
    "    # \n",
    "    def hash(self, x, h):\n",
    "        # If the input vector x has d dimensions, we add an extra dimension of 0.\n",
    "        # This is needed for the query vectors, but not the key vectors because\n",
    "        # we already added an extra dimension to them.\n",
    "        if len(x) == self.d - 1:\n",
    "            # print(\"Extending x\")\n",
    "            x = torch.cat((x, torch.tensor([0.0])))\n",
    "\n",
    "        hash_value = 0\n",
    "        for i in range(self.k):\n",
    "            # print(f\"Dot product: {x} and {h[i]}\")\n",
    "            if torch.dot(x, h[i]) >= 0:\n",
    "                hash_value += 2**i\n",
    "\n",
    "        return hash_value\n",
    "\n",
    "    # Function to query how many (and which) key vectors are in the same bucket \n",
    "    # as the query vector q.\n",
    "    # \n",
    "    # We limit the number of returned key vectors to max_results.\n",
    "    # We also require the keys to have a dot product of at most cr with the query vector.\n",
    "    #\n",
    "    # Runtime: O(max(max_results, number of keys in the bucket))\n",
    "    def query_bucket_size(self, q, max_results):\n",
    "\n",
    "        # If the query vector q has d dimensions, we add an extra dimension of 0.\n",
    "        # print(f\"Length of q: {len(q)} vs {self.d}\")\n",
    "        # print(q)\n",
    "        if len(q) == self.d - 1:\n",
    "            # print(\"Extending q\")\n",
    "            q = torch.cat((q, torch.tensor([0.0])))\n",
    "\n",
    "        # Normlize q to have norm 1.\n",
    "        q = q / torch.norm(q)\n",
    "\n",
    "        distinct_keys = set()\n",
    "        for j in range(self.L):\n",
    "            hj = self.hash(q, self.hash_vectors[j])\n",
    "\n",
    "            print(f\"Query in hash table {j} was hashed at bucket {hj}\")\n",
    "\n",
    "            if hj in self.hash_tables[j]:\n",
    "                for key_index in self.hash_tables[j][hj]:\n",
    "\n",
    "                    # Check if the key vector has a dot product of at most cr with the query vector.\n",
    "                    if torch.dot(q, self.K[key_index]) >= self.b:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add the key index to the set of distinct keys.\n",
    "                    distinct_keys.add(key_index)\n",
    "\n",
    "                    # If we have reached the maximum number of results, return the keys.\n",
    "                    if len(distinct_keys) >= max_results:\n",
    "                        return distinct_keys, len(distinct_keys)\n",
    "\n",
    "        return distinct_keys, len(distinct_keys)\n",
    "    \n",
    "    def print_buckets(self):\n",
    "        print(f\"Printing {self.L} hash tables:\")\n",
    "        for j in range(self.L):\n",
    "            print(\"Hash table\", j)\n",
    "            for key in self.hash_tables[j]:\n",
    "                print(key, \":\", self.hash_tables[j][key])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a sequence of concentric LSH objects with increasing r values.\n",
    "def topk_indices_lsh_preprocessing(K, B):\n",
    "    n, d = K.size()\n",
    "\n",
    "    # !! Note !! - c requires tweaking! \n",
    "    c = 0.2\n",
    "    current_s = 0\n",
    "\n",
    "    print(f\"Creating {int(1/(c)) - 1} LSH objects.\")\n",
    "\n",
    "    # We will create a sequence of LSH objects with increasing r values.\n",
    "    lsh_objects = []\n",
    "    while current_s < 1 - 2*c:\n",
    "        current_s += c\n",
    "\n",
    "        lsh_objects.append(angularLSH(K, s=current_s, b=current_s+c, B=B))\n",
    "\n",
    "    return lsh_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the top-k inner products q^T k_i for a query q and a set of \n",
    "# keys K. It does so in O(k) time by using the concentric circle LSH idea.\n",
    "#\n",
    "# Requires topk_indices_lsh_preprocessing() to be run first.\n",
    "# \n",
    "# Parameters:\n",
    "#   q is the query vector: 1 x d\n",
    "#   K is the matrix of key vectors: n x d\n",
    "#   k is the number of top-k elements to consider\n",
    "#   B is the maximum value of the q,k,v elements. We use this to avoid numerical instability.\n",
    "#   lsh_objects is a list of LSH objects with increasing s values.\n",
    "def topk_indices_fast_lsh(q, K, k, B, lsh_objects, verbose=False):\n",
    "\n",
    "    n, d = K.size()\n",
    "\n",
    "    q_copy = q.clone().float()\n",
    "    lsh_before = 0\n",
    "    keys_before = set()\n",
    "    lsh_after = 0\n",
    "    keys_after = set()\n",
    "\n",
    "    found = False\n",
    "    # Find the first LSH object where at least k keys are hashed to the same bucket as the query vector.\n",
    "    for i in range(len(lsh_objects)):\n",
    "        keys, num_keys = lsh_objects[i].query_bucket_size(q, k)\n",
    "        print(\"Lsh object\", i, \"has\", num_keys, \"keys. These are: \", keys)\n",
    "        if num_keys >= k:\n",
    "            found = True\n",
    "            lsh_before = i - 1\n",
    "            lsh_after = i\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        print(\"Error: No LSH object found with at least k keys in the same bucket as the query vector.\")\n",
    "        return\n",
    "    # Now we have two LSH objects, so that the first one has less than k keys\n",
    "    # hashed to the same bucket as the query vector, and the second one has at least k keys.\n",
    "    # We will take all the keys from the first one and supplement with keys from the second one.\n",
    "\n",
    "    if lsh_after == 0:\n",
    "        keys_after, _ = lsh_objects[0].query_bucket_size(q, k)\n",
    "    else:\n",
    "        # Find the keys from the first LSH object.\n",
    "        keys_before, _ = lsh_objects[lsh_before].query_bucket_size(q, k)\n",
    "        keys_after, _ = lsh_objects[lsh_after].query_bucket_size(q, k)\n",
    "\n",
    "    if verbose:\n",
    "        print(keys_before)\n",
    "        print(keys_after)\n",
    "\n",
    "    keys = keys_before\n",
    "    for key in keys_after:\n",
    "        if key not in keys:\n",
    "            keys.add(key)\n",
    "\n",
    "        if len(keys) >= k:\n",
    "            break\n",
    "\n",
    "    # Now we have the top-k keys. We will calculate the attention scores for these keys.\n",
    "    attention_scores = torch.zeros(len(keys))\n",
    "    for i, key in enumerate(keys):\n",
    "        attention_scores[i] = torch.dot(q_copy, (K[key, :]/d)) - B*B\n",
    "        attention_scores[i] = torch.exp(attention_scores[i])\n",
    "\n",
    "    return keys, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive answer:\n",
      "(tensor([[0.3679, 0.2231]]), tensor([[1, 0]]))\n",
      "\n",
      "Creating 4 LSH objects.\n",
      "Power of L: 0.804258719533377\n",
      "Creating 2 hash tables with 2 bits each.\n",
      "Power of L: 0.75965463755723\n",
      "Creating 2 hash tables with 2 bits each.\n",
      "Power of L: 0.6552495395524321\n",
      "Creating 2 hash tables with 2 bits each.\n",
      "\n",
      "---\n",
      "Created 3 LSH objects.\n",
      "---\n",
      "\n",
      "Printing 2 hash tables:\n",
      "Hash table 0\n",
      "3 : [0, 1, 2]\n",
      "\n",
      "Hash table 1\n",
      "3 : [0, 1, 2]\n",
      "\n",
      "Printing 2 hash tables:\n",
      "Hash table 0\n",
      "1 : [0, 1, 2]\n",
      "\n",
      "Hash table 1\n",
      "2 : [0, 1, 2]\n",
      "\n",
      "Printing 2 hash tables:\n",
      "Hash table 0\n",
      "3 : [0, 1, 2]\n",
      "\n",
      "Hash table 1\n",
      "0 : [0, 1, 2]\n",
      "\n",
      "Query in hash table 0 was hashed at bucket 1\n",
      "Query in hash table 1 was hashed at bucket 1\n",
      "Lsh object 0 has 0 keys. These are:  set()\n",
      "Query in hash table 0 was hashed at bucket 3\n",
      "Query in hash table 1 was hashed at bucket 1\n",
      "Lsh object 1 has 0 keys. These are:  set()\n",
      "Query in hash table 0 was hashed at bucket 3\n",
      "Lsh object 2 has 2 keys. These are:  {0, 1}\n",
      "Query in hash table 0 was hashed at bucket 3\n",
      "Query in hash table 1 was hashed at bucket 1\n",
      "Query in hash table 0 was hashed at bucket 3\n",
      "({0, 1}, tensor([0.2231, 0.3679]))\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "q = torch.tensor([[1, 0, 0, 1]])\n",
    "K = torch.tensor([[1, 2, 3, 9], [4, 5, 6, 8], [7, 8, 9, 1]])\n",
    "\n",
    "print(\"Naive answer:\")\n",
    "print(topk_indices_naive(q, K, 2, 2))\n",
    "print()\n",
    "\n",
    "lsh_objects = topk_indices_lsh_preprocessing(K, B=10.0)\n",
    "\n",
    "assert len(lsh_objects) != 0, \"Should create at least one LSH object.\" \n",
    "\n",
    "print(f\"\\n---\\nCreated {len(lsh_objects)} LSH objects.\\n---\\n\")\n",
    "\n",
    "for lsh_object in lsh_objects:\n",
    "    lsh_object.print_buckets()\n",
    "\n",
    "print(topk_indices_fast_lsh(q[0], K, 2, 2.0, lsh_objects, verbose=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 4 LSH objects.\n",
      "Power of L: 0.804258719533377\n",
      "Creating 70 hash tables with 8 bits each.\n",
      "Power of L: 0.75965463755723\n",
      "Creating 55 hash tables with 8 bits each.\n",
      "Power of L: 0.6552495395524321\n",
      "Creating 32 hash tables with 8 bits each.\n",
      "Query in hash table 0 was hashed at bucket 58\n",
      "Query in hash table 1 was hashed at bucket 35\n",
      "Query in hash table 2 was hashed at bucket 71\n",
      "Query in hash table 3 was hashed at bucket 179\n",
      "Query in hash table 4 was hashed at bucket 98\n",
      "Query in hash table 5 was hashed at bucket 176\n",
      "Query in hash table 6 was hashed at bucket 72\n",
      "Query in hash table 7 was hashed at bucket 56\n",
      "Query in hash table 8 was hashed at bucket 41\n",
      "Query in hash table 9 was hashed at bucket 184\n",
      "Query in hash table 10 was hashed at bucket 69\n",
      "Query in hash table 11 was hashed at bucket 204\n",
      "Query in hash table 12 was hashed at bucket 204\n",
      "Query in hash table 13 was hashed at bucket 125\n",
      "Query in hash table 14 was hashed at bucket 128\n",
      "Query in hash table 15 was hashed at bucket 159\n",
      "Query in hash table 16 was hashed at bucket 134\n",
      "Query in hash table 17 was hashed at bucket 204\n",
      "Query in hash table 18 was hashed at bucket 233\n",
      "Query in hash table 19 was hashed at bucket 22\n",
      "Query in hash table 20 was hashed at bucket 39\n",
      "Query in hash table 21 was hashed at bucket 18\n",
      "Query in hash table 22 was hashed at bucket 207\n",
      "Query in hash table 23 was hashed at bucket 134\n",
      "Query in hash table 24 was hashed at bucket 231\n",
      "Query in hash table 25 was hashed at bucket 178\n",
      "Query in hash table 26 was hashed at bucket 43\n",
      "Query in hash table 27 was hashed at bucket 43\n",
      "Query in hash table 28 was hashed at bucket 185\n",
      "Query in hash table 29 was hashed at bucket 18\n",
      "Query in hash table 30 was hashed at bucket 48\n",
      "Query in hash table 31 was hashed at bucket 214\n",
      "Query in hash table 32 was hashed at bucket 191\n",
      "Query in hash table 33 was hashed at bucket 128\n",
      "Query in hash table 34 was hashed at bucket 23\n",
      "Query in hash table 35 was hashed at bucket 171\n",
      "Query in hash table 36 was hashed at bucket 128\n",
      "Query in hash table 37 was hashed at bucket 129\n",
      "Query in hash table 38 was hashed at bucket 100\n",
      "Query in hash table 39 was hashed at bucket 158\n",
      "Query in hash table 40 was hashed at bucket 237\n",
      "Query in hash table 41 was hashed at bucket 2\n",
      "Query in hash table 42 was hashed at bucket 252\n",
      "Query in hash table 43 was hashed at bucket 137\n",
      "Query in hash table 44 was hashed at bucket 31\n",
      "Query in hash table 45 was hashed at bucket 100\n",
      "Query in hash table 46 was hashed at bucket 14\n",
      "Query in hash table 47 was hashed at bucket 210\n",
      "Query in hash table 48 was hashed at bucket 199\n",
      "Query in hash table 49 was hashed at bucket 55\n",
      "Query in hash table 50 was hashed at bucket 73\n",
      "Query in hash table 51 was hashed at bucket 122\n",
      "Query in hash table 52 was hashed at bucket 252\n",
      "Query in hash table 53 was hashed at bucket 219\n",
      "Query in hash table 54 was hashed at bucket 164\n",
      "Query in hash table 55 was hashed at bucket 117\n",
      "Query in hash table 56 was hashed at bucket 74\n",
      "Query in hash table 57 was hashed at bucket 255\n",
      "Query in hash table 58 was hashed at bucket 223\n",
      "Query in hash table 59 was hashed at bucket 99\n",
      "Query in hash table 60 was hashed at bucket 188\n",
      "Query in hash table 61 was hashed at bucket 116\n",
      "Query in hash table 62 was hashed at bucket 5\n",
      "Query in hash table 63 was hashed at bucket 76\n",
      "Query in hash table 64 was hashed at bucket 235\n",
      "Query in hash table 65 was hashed at bucket 208\n",
      "Query in hash table 66 was hashed at bucket 117\n",
      "Query in hash table 67 was hashed at bucket 245\n",
      "Query in hash table 68 was hashed at bucket 94\n",
      "Query in hash table 69 was hashed at bucket 53\n",
      "Lsh object 0 has 0 keys. These are:  set()\n",
      "Query in hash table 0 was hashed at bucket 105\n",
      "Query in hash table 1 was hashed at bucket 231\n",
      "Query in hash table 2 was hashed at bucket 78\n",
      "Query in hash table 3 was hashed at bucket 209\n",
      "Query in hash table 4 was hashed at bucket 249\n",
      "Query in hash table 5 was hashed at bucket 184\n",
      "Query in hash table 6 was hashed at bucket 86\n",
      "Query in hash table 7 was hashed at bucket 120\n",
      "Query in hash table 8 was hashed at bucket 163\n",
      "Query in hash table 9 was hashed at bucket 214\n",
      "Query in hash table 10 was hashed at bucket 17\n",
      "Query in hash table 11 was hashed at bucket 34\n",
      "Query in hash table 12 was hashed at bucket 105\n",
      "Query in hash table 13 was hashed at bucket 41\n",
      "Query in hash table 14 was hashed at bucket 69\n",
      "Query in hash table 15 was hashed at bucket 151\n",
      "Query in hash table 16 was hashed at bucket 168\n",
      "Query in hash table 17 was hashed at bucket 219\n",
      "Query in hash table 18 was hashed at bucket 201\n",
      "Query in hash table 19 was hashed at bucket 207\n",
      "Query in hash table 20 was hashed at bucket 0\n",
      "Query in hash table 21 was hashed at bucket 194\n",
      "Query in hash table 22 was hashed at bucket 208\n",
      "Query in hash table 23 was hashed at bucket 169\n",
      "Query in hash table 24 was hashed at bucket 127\n",
      "Query in hash table 25 was hashed at bucket 209\n",
      "Query in hash table 26 was hashed at bucket 171\n",
      "Query in hash table 27 was hashed at bucket 62\n",
      "Query in hash table 28 was hashed at bucket 20\n",
      "Query in hash table 29 was hashed at bucket 12\n",
      "Query in hash table 30 was hashed at bucket 144\n",
      "Query in hash table 31 was hashed at bucket 182\n",
      "Query in hash table 32 was hashed at bucket 98\n",
      "Query in hash table 33 was hashed at bucket 6\n",
      "Query in hash table 34 was hashed at bucket 37\n",
      "Query in hash table 35 was hashed at bucket 6\n",
      "Query in hash table 36 was hashed at bucket 15\n",
      "Query in hash table 37 was hashed at bucket 149\n",
      "Query in hash table 38 was hashed at bucket 124\n",
      "Query in hash table 39 was hashed at bucket 69\n",
      "Query in hash table 40 was hashed at bucket 46\n",
      "Query in hash table 41 was hashed at bucket 96\n",
      "Query in hash table 42 was hashed at bucket 141\n",
      "Query in hash table 43 was hashed at bucket 145\n",
      "Query in hash table 44 was hashed at bucket 56\n",
      "Query in hash table 45 was hashed at bucket 220\n",
      "Query in hash table 46 was hashed at bucket 100\n",
      "Query in hash table 47 was hashed at bucket 205\n",
      "Query in hash table 48 was hashed at bucket 120\n",
      "Query in hash table 49 was hashed at bucket 83\n",
      "Query in hash table 50 was hashed at bucket 222\n",
      "Query in hash table 51 was hashed at bucket 253\n",
      "Query in hash table 52 was hashed at bucket 122\n",
      "Query in hash table 53 was hashed at bucket 218\n",
      "Query in hash table 54 was hashed at bucket 188\n",
      "Lsh object 1 has 0 keys. These are:  set()\n",
      "Query in hash table 0 was hashed at bucket 249\n",
      "Query in hash table 1 was hashed at bucket 205\n",
      "Query in hash table 2 was hashed at bucket 129\n",
      "Query in hash table 3 was hashed at bucket 31\n",
      "Query in hash table 4 was hashed at bucket 118\n",
      "Query in hash table 5 was hashed at bucket 118\n",
      "Query in hash table 6 was hashed at bucket 167\n",
      "Query in hash table 7 was hashed at bucket 221\n",
      "Query in hash table 8 was hashed at bucket 107\n",
      "Query in hash table 9 was hashed at bucket 51\n",
      "Query in hash table 10 was hashed at bucket 12\n",
      "Query in hash table 11 was hashed at bucket 244\n",
      "Query in hash table 12 was hashed at bucket 165\n",
      "Query in hash table 13 was hashed at bucket 198\n",
      "Query in hash table 14 was hashed at bucket 10\n",
      "Query in hash table 15 was hashed at bucket 58\n",
      "Query in hash table 16 was hashed at bucket 44\n",
      "Query in hash table 17 was hashed at bucket 33\n",
      "Query in hash table 18 was hashed at bucket 252\n",
      "Query in hash table 19 was hashed at bucket 128\n",
      "Query in hash table 20 was hashed at bucket 141\n",
      "Query in hash table 21 was hashed at bucket 15\n",
      "Query in hash table 22 was hashed at bucket 225\n",
      "Query in hash table 23 was hashed at bucket 246\n",
      "Query in hash table 24 was hashed at bucket 130\n",
      "Query in hash table 25 was hashed at bucket 2\n",
      "Query in hash table 26 was hashed at bucket 82\n",
      "Query in hash table 27 was hashed at bucket 89\n",
      "Query in hash table 28 was hashed at bucket 2\n",
      "Query in hash table 29 was hashed at bucket 78\n",
      "Query in hash table 30 was hashed at bucket 148\n",
      "Query in hash table 31 was hashed at bucket 27\n",
      "Lsh object 2 has 0 keys. These are:  set()\n",
      "Error: No LSH object found with at least k keys in the same bucket as the query vector.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m lsh_objects \u001b[38;5;241m=\u001b[39m topk_indices_lsh_preprocessing(K, B\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[43msampling_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_indices_fast_lsh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlsh_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlsh_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for approximate attention:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_time \u001b[38;5;241m-\u001b[39m start_time)\n",
      "Cell \u001b[0;32mIn[167], line 20\u001b[0m, in \u001b[0;36msampling_attention\u001b[0;34m(Q, K, V, k, l, topk_indices_func, B, lsh_objects)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)): \u001b[38;5;66;03m# n\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# For all columns in V...\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(V\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)): \u001b[38;5;66;03m# d\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Approximate the expected value of the value vector\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         output[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mapproximate_softmax_expectation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_indices_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlsh_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[166], line 20\u001b[0m, in \u001b[0;36mapproximate_softmax_expectation\u001b[0;34m(q, K, v, k, l, topk_indices_func, B, lsh_objects)\u001b[0m\n\u001b[1;32m     17\u001b[0m n, d \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Find the top-k indices of the attention scores\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m topk_indices_func(q, K, k, B, lsh_objects)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# From the n-k remaining elements, draw l samples.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# TODO: Implement this in O(l) time.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m remaining_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(indices)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate 50x50 random matrices with elements in range [-10, 10]\n",
    "Q = torch.randint(-10, 11, (200, 200)).float()\n",
    "K = torch.randint(-10, 11, (200, 200)).float()\n",
    "V = torch.randint(-10, 11, (200, 200)).float()\n",
    "\n",
    "# Call the sampling_attention function.\n",
    "# Measure the time it takes to run the function.\n",
    "start_time = time.time()\n",
    "lsh_objects = topk_indices_lsh_preprocessing(K, B=10.0)\n",
    "attention_output = sampling_attention(Q, K, V, 50, 50, topk_indices_fast_lsh, B=10.0, lsh_objects=lsh_objects)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for approximate attention:\", end_time - start_time)\n",
    "\n",
    "# Compare with the exact attention output\n",
    "# Calculate the time taken to run the calculate_attention function.\n",
    "start_time = time.time()\n",
    "\n",
    "# First run the preprocessing step to create the LSH objects.\n",
    "exact_attention_output = calculate_attention(Q, K, V, B=10.0)\n",
    "end_time = time.time()\n",
    "print(\"Time taken for exact attention:\", end_time - start_time)\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(\"Mean error: \", torch.mean(torch.abs(attention_output - exact_attention_output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
